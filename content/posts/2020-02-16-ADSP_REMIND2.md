---
title : "ADSP 복습 2"
category :
    - ADSP
tag :
    - ADSP
toc : true
---

# 데이터 분석
## R기초와 데이터 마트

### R언어와 문법

#### 데이터 구조
- 벡터 : 하나의 스칼라값, 하나 이상의 스칼라 원소(동일한 자료형)
- 행렬 -> matrix
- 데이터 프레임 -> data.frame(incom=a1,car=b1)
- 배열 -> array(1:12, dim=c(3,4))
- 리스트 : list(name="a",height=123)

#### 기초 함수
- solve : 역행렬
- cov, cor : 공분산, 상관계수
- substr(char,num1,num2) : num1번째에서 num2번째 계산
- format(sys.Date(),'%a') : 현재 요일 출력

<br>

- pairs : 산점도 행렬
- melt(data,id) : id기준으로 variable, value 저장
- cast(data,day~month~variable) : day, month기준으로 변수 배열
- acast(data,month~variable,mean) : month 당 평균
- subset 등으로 특정 변수만 선택가능
- sqldf("쿼리문")


### 결측치 처리와 이상값 검색
#### 결측값 대치법
1. 평균 대치법 : 적절한 평균값으로 결측값 대치
2. 단순확률 대치법 : 평균대치법에서 표준오차의 과소추정 문제를 보완, 적절한 확률값 부여 후 대치
3. 다중 대치법 : 통계량 효율성 및 일치성 문제 보완, 과소 추정 문제는 여전
- complete.cases() : 행의 모든 값이 na아닌 경우 True

이상값 : boxplot 보기

## 통계분석
### 통계학개론
**확률적 추출**
1. 단순 무작위 추출 : 임의로 개수 뽑기
2. 계통추출 : 일정 간격으로 표본 추출
3. 층화추출 : 집단, 층으로 나누고 집단 내 원하는 크기의 표본 무작위 추출
4. 군집추출 : 집단을 나눠 집단 선택해 표본 추출

**비확률적 추출**
1. 판단추출 : 연구자의 판단으로 추출
2. 할당추출 : 여러 집단으로 나눠 각 집단에서 연구자의 판단으로 추출
3. 편의추출 : 쉽게 접근할 수 있는 표본 추출

자료 : 명목 서열 등간 비율

**좋은 추정량 조건** 
불편성, 효율성, 충족성, 일관성

#### 검정 방법
1. 모수적 검정
- 가정된 분포의 모수에 대해 가설 설정
- 표본평균, 분산 등을 구해 검정을 실시

2. 비모수적 검정
- 분포에 제약을 가정x, 특정 분포를 따른다고 가정할 수 없는 경우
- 가설은 단지 분포의 형태가 동일하다 아니다로 분포의 형태에 대해 설정
- 순위나 부호 등을 이용해 검정
- 부호검정,윌콕슨, 부호순위합, 스피어만 순위상관, 만 위트니

### 회귀분석
**가정**
1. 선형성
2. 독립성
3. 등분산성
4. 비상관성
5. 정상성(잔차항이 정규분포)

- Residuals vs Fitted는 y축의 잔차를 보여줌, 기울기가 0인 직선이 이상적
- noraml Q-Q는 잔차가 정규분포를 따르는지 확인, 직선 상에 있어야함
- Scale Location은 y축의 표준화 잔차를 나타냄, 기울기가 0인 직선이 이상적
- Residuals vs Leverage에서 cook's distance가 1이 넘어가면 영향점이라 판단

#### 다중공선성
- 변수들이 상관되어 있을 때 발생, vif가 4가 넘으면 다중공선성이 존재한다고 봄

#### 최적으 회귀방정식 선택
- aic, bic로 적합성을 봄
- 단계적 변수선택법

```r
step(lm(y~1,df),scope=list(lower=~1,upper=~x1+x2),direction="forward")
```

릿지회귀 : l2 norm 최소화
라쏘회귀 : l1 norm 최소화
엘라스틱넷 : 위의 두 개 절충

### 다변량 분석
1. 상관분석
서열척도 - 스피어만 상관분석
등간, 비율척도 - 피어슨 상관, 편상관분석

cor.test() : 상관계수 검정

2. 다차원 척도법(MDS)
- 유사성을 측정해 2, 3차원 공간에 표현

### 주성분 분석
- 차원 축소 기법
- biplot : 2개의 주성분만 2차원의 그래프로 표현

### 시계열 분석
1. 정상성
- 분산과 수준에 체계적 변화가 없고 주기적 변동이 없음, 미래는 확률적으로 과거와 동일

2. 비정상성시계열을 정상성으로 만들기
- 평균이 일정하지 않은 경우 차분을 하면 됨
- 계절성을 가질 경우 계절차분 사용
- 분산이 일정하지 않은 경우 로그변환 등 진행

**백색잡음 과정** : 평균이 0, 분산이 일정, 자기공분산 0
**자기상관** : t, t-1간 상관 관계

### 시게열 모형
1. 자기회귀모형(AR) : 현시점의 자료에 몇 번째 전 자료까지 영향을 주는가
- AR(1) 모형은 백색잡음값과 1스텝 과거의 자기 자신의 값만의 가중합임
- 자기상관함수 : 점차적 감소
- 부분자기상관함수 : 급격히 감소해 절단

2. 이동평균모형(MA)
- 데이터의 평균을 예측치로 사용, 동일한 가중치
- 항성 정상성 만족
- 자기상관함수 : 급격히 감소해 절단

3. 자기회귀누적이동 모형(ARIMA)
- 차분과 변환을 통해 정상화
- ARIMA(p,d,q)에서
    - d=0 -> ARMA(p,q)
    - p는 AR, q는 MA와 관련
- 두 개의 상관함수가 지수적 감소

4. 분해 시계열
- 추세요인 : 특정한 형태
- 계절요인 : 고정된 주기에 따라 변화
- 순환요인 : 알려지지 않은 주기를 가지고 자료가 변화
- 불규칙요인 : 설명할 수 없는 요인(오차)

```r
diff(data,differences=1)
# 한번 차분
decompose(data)
# 시계열을 4가지 요인으로 분해
acf(data,lag.max=20)
pacf(data,lag.max=20)
# 자기상관함수와 부분자기상관함수
auto.arima(data)
# 적절한 ARIMA모형 결정
```

## 정형 데이터 마이닝
### 분류분석
#### 로지스틱 회귀
- 오즈비 : 성공율/실패율

|구분|선형 회귀분석|로지스틱 회귀분석|
|:-:|:-:|:-:|
|종속변수|연속형|이산형|
|모형 탐색 방법|최소자승법|최대우도법, 가중최소자승법|
|모형 검정|F-test, t-test|카이제곱 test|

로짓변환 : log(p/(1-p))

최대우도 추정법
```r
glm(pmale~x,family='binomial',weight=total)
```

estimate값의 exp값이 오즈의 배수가 됨

### 신경망 모형
활성함수 : 시그모이드, 부호, 소프트맥스

### 의사결정나무
부모마디 : 상위마디  자식마디 : 하위마디
뿌리마디 : 맨 위의 마디    최종마디 : 더 이상 분할되지 않음

#### 분류나무
- 지니지수 : 1-p^2의 합으로 계산
- 엔트로피 지수 : -plogp의 합으로 계산

#### 회귀나무
- F통계량의 p값, 분산감소량이 분류 기준값의 선택 방법이 됨

**기준변수의 선택법**

|구분|이산형|연속형|
|:-:|:-:|:-:|
|CHAID|카이제곱통계량|ANOVA 통계량|
|CART|지니 지수|분산 감소량|
|C4.5|엔트로피 지수|X|

- 장점
    - 해석이 용이. 수학적 가정이 불필요한 비모수적 모형
    - 계산 비용 낮음, 상호작용 및 비선형성 고려가능
- 단점
    - 경계선 부근에서 오차가 큼
    - 각 예측변수의 효과를 파악하기 어려움

### 양상블 모형
1. 배깅 : 임의 복원추출해 각 표본에 대해 분류기를 생성 후 결과를 앙상블

2. 부스팅 : 동일한 확률이 아닌 분류가 잘못된 데이터에 가중치를 주어 표본 추출

3. 랜덤 포레스트 : 배깅 + 랜덤

### SVM
- 데이터 간 간격이 최대가 되는 선을 찾아 기준으로 데이터 분류
- 장점
    - 에러율 낮고 결과 해석 용이
- 단점
    - 파라미터 선택에 민감, 이진 분류만 가능
- COST로 비용의 합 최소화하는 선 찾음

### 나이브 베이즈
- 사후확률은 사전확률을 통해 예측할 수 있다.
- 장점
    - TRAINING 데이터가 적어도 사용가능
    - multi-class를 쉽고 빠르게 예측
- 단점
    - 한 쪽에 없는 경우 정상적 예측 불가
    - 확률적으로 독립이라는 가정이 위반되면 오류 발생

### 모형평가
#### 홀드아웃
- train과 test 나눠서 실행
#### 교차 검증
- K개로 나눠 K-1를 훈현, 1개를 테스트로 하여 K번 반복
#### 붓스트랩
1. 오분류표 : 분류에서 사용

2. ROC그래프
- X축에는 FP Ratio(1-특이도), y축에는 민감도를 나타냄
- 면적이 넓을 수록 좋은 모형

3. 이익도표와 향상도
- 이익 : 개체들이 각 등급에 얼마나 분포하는가, 이익값을 누적으로 연결하면 이익도표
- 향상도 곡선 : 모델의 성과가 얼마나 향상되었나

### 군집분석
1. 계층적 군짐
- 단일 연결법 : 짧은 거리로 고립된 군집을 찾음
- 완전 연결법 : 거리의 최댓값, 내부 응집성 중심
- 평균연결법 : 거리 평균으로 군집화, 계산량이 많음
- 중심연결법 : 중심간 거리, 가중 평균
- 와드연결법 : 오차제곱합 기초

**거리 측정**
유클리드 : 거리 제곱합의 제곱근
맨하튼 : 절대값의 합
민코프스키 거리 : q=2면 유클리드, q=1이면 맨하튼

마할라노비스 : 표준화와 상관성을 고려한 통계적 거리

2. 비계층적 군집
1. k-mean
    1) k개 임의 선택
    2) 군집의 중심점으로부터 오차제곱합이 최소가 되로록 각 자료 할당
    3) 군집 내 평균을 계산해 군집 중심 갱신
    4) 군집 중심 변화가 없을 때까지 2,3 반복
- 장점 
    - 단순, 많은 양 처리
    - 사전적 정보 없이 의미있는 자료 분석 가능
- 단점
    - k값을 정해야 함
    - 잡음이나 이상값에 영향받기 쉬움(평균 대신 중앙값을 사용하는 k-medoids 사용)

3. 혼합분포 군집
- EM 알고리즘 : 혼합분포에서 잠재변수를 추정할 때 사용
    1) K개의 클러스터 초기화
    2) 포인터가 클러스터에 포함될 확률 계산
    3) MSL이 최대화 되기 위한 파라미터 계산

- kmean은 중심거리 EM은 MSL로 거리를 측정

4. SOM(자기조직화지도)
- 인공신경망의 종류, 차원축소와 군집화를 동시에 시행
- SOM 프로세스
    1. 연결 강도 초기화
    2. 입력 벡터와 경쟁층 노드 간의 유클리드 거리 계산
    3. 선택 노드와 이웃 노드 가중치 수정
    4. 2로 가서 반복, 입력 패턴과 가장 유사한 경쟁층 뉴런이 승자가 됨
- 승자 독식 구조로 경쟁층에는 승자 뉴런만이 나타남

- 신경망은 에러 수정, SOM은 경쟁 학습
- SOM은 비지도 학습

### 연관분석
지지도 : 교집합
신뢰도 : 조건부 확률
향상도 : 교집합/a확률, b확률

- 향상도가 1보다 크면 양의 관계, 1이면 연관성 없음

절차
- 최소 지지도 설정
- 최소 지지도를 넘는 두 가지 품목 찾음
- 최소 지지도를 넘는 세 가지 품목 찾음
- 반복 수행

**Apriori 알고리즘**
- 데이터 간의 연관관계
- 구매 패턴 등 분석

- 장점
    - 결과 이해 쉬움, 비목적성 분석 기법
    - 사용 편리, 계산 간편
- 단점
    - 품목 수가 증가하면 계산량 증가
    - 너무 세부화되면 의미없는 결과 도출
    - 상대적 거래량이 적으면 제외되기 쉽다.
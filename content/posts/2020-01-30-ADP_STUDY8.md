---
title : "ADP 공부하기 8"
category :
    - ADP
tag :
    - ADP
toc : true
---

# 분산 컴퓨팅 기술

### MapReduce
개념
- 구글에서 **분산 병렬 컴퓨팅**을 이용하여 2004년 논문에서 공개됨
- 분할정복 방식으로 **대용량 데이터를 병렬로 처리**할 수 있는 모델
    - 분할정복 : 성질이 같은 여러 부분으로 나눠 해결한 뒤 원래 문제의 해를 구함
- c++,JAVA 적용, 아파치 하둡의 Hadoop MapReduce가 동일한 기능
- 클라이언트의 작업 단위는 맵리듀스 잡 map, reuce task로 나뉨
- **map task 하나가 1개의 블록을 대상으로 연산, 사용자가 지정한 개수에 해당하는 reduce task들이 받아** 정렬 및 필터링 작업을 거침


### 구글 맵리듀스
- 복잡성을 추상화시켜 핵심 기능 구현에만 집중하게 함
- map에는 **key와 value 쌍들을 입력**으로 받음
- map함수를 거치면서 다수의 새로운 key, value로 변환
- reduce로 전동됨. 이 과정에서 **shuffle과 group by 정렬** 과정을 거쳐 **reduce의 입력 레코드**로 들어가게 되는데 형식은 **key, value의 리스트**다
- reduce 함수를 통해 최종 output 산출, 
- 사용자는 map과 reduce 함수만 작성

#### 실행 과정
- 마스터는 입력 데이터소스를 가지고 스케줄링함
- **각 split이 map 프로세스들의 할당 단위**, split은 64~128mb(블록 단위), **split 수만큼 map task들이 워커로부터 fork됨**
- output값은 **partitionar**라는 reduce번호를 할당해 주는 클래스를 통해 보낼지 정함.정해지지 않으면 **동일한 key들은 같은 reduce로 배정**
= map단계가 끝나면 **원격의 reduce 워커들이 자기에 할당된 map의 중간 값들을 네트워크로 가져**, 사용자의 reduce로직을 통해 산출물
- 보통 reduce의 개수는 map의 개수보다 적고, map의 중간 데이터 사이즈에 따라 성능이 좌우됨
- 적합한 경우 : 분산grep이나 빈도수 계산 등의 작업
    - map단계를 거치며 데이터 사이즈가 크게 줄어들고 줄어든 크기만큼 reduce의 오버헤드도 줄어듬
= 적합하지 않은 경우 : 정렬과 같은 작업
    - 사이즈 줄지 않고 오버헤드에 따라 수행 성능이 저하

#### 폴트톨러런스
- **각 프로세스에서는 master에게 task진행 상태를 주기적으로 보냄**
- 마스터는 워커들의 task 상태 정보를 가지고 있다가 특정 워커가 진행하지 않고 정보를 받지 못하면 task에 문제가 있다고 판정
- 복구를 할 때 map, reduce task들이 죽은 경우 해당 task가 데이터 정보만 다른 워커에게 전하면 새로운 task를 실행하면 됨.
- **mapreduce는 shared nothing 아키텍쳐**

### Hadoop MapReduce
- 아파치 오픈소스 프로젝트, java로 구현
- HDFS와 Hadoop MapReduce가 하둡의 핵심 구성요소

#### 아키텍쳐
- 하둡은 데몬 관점에서 4개의 구성 요소를 가짐

|구분|설명|
|:-:|-|
|네임노드|하둡을 이루는 필수적인 데몬, 마스터 역할 수행|
|데이터노드|분산 파일 시스템의 데몬, 데이터 입출력에 대한 처리 수행|
|잡트래커|시스템에서 job이라는 작업을 관리하는 마스터에 해당|
|태스크트래커|작업을 수행하는 워커 데몬, 슬레이브에 해당(각 노드에 1개의 태스크 트래커)|

- **클라이언트에서 잡이라 부르는 하둡 작업을 실행** -> 환경 정보들이 jobtracker에 전송
- jobtracker는 작업을 다수의 task로 쪼개고 데이터 지역성을 보장하기 위해 task를 어떤 tasktracker에게 보낼지를 감안해 내부적으로 스케줄링해 큐에 저장
- 이 때 **task는 맵퍼나 리듀서가 수행하는 단위 작업**
- tasktracker는 jobtracker에게 3초에 한 번씩 주기적으로 하트비트 보냄
- tasktracker에서 하트비트를 보내면 jobtracker는 할당된 task가 있는지 큐에서 확인 후, 있으면 response메세지에 task정보를 실어 tasktracker에 보냄
- tasktracker는 메시지의 내용을 분석해 프로세스를 fork함


#### 실행 절차
1. 스플릿 : 파일 스플릿 생성, **파일 스플릿 하나당 map task 하나씩 생성**
2. 맵 : 각 split에 대해 레코드 단위로 map함수 적용, **key-value쌍 생성**
3. 컴바인 : 리듀스와 동일한 프로그램 적용, 데이터의 크기를 줄임
4. 파티션 : key를 기준으로 데이터를 디스크에 분할 저장, 정렬 수행, 분할된 파일들은 각각 다른 reduce task에 저장
5. 셔플 : 맵퍼들의 결과 파일을 각 리듀서에 할당, 할당된 파일을 로컬 파일 시스템으로 복사
6. 정렬 : 병합 정렬 방식으로 맵퍼의 결과 파일을 key기준으로 정렬
7. 리듀스 : 리듀스 함수 적용

- 기본적으로 output format은 key,value를 탭으로 구분하며, mapred.textoutputformat.separator 속성을 사용해 구분자를 원하는 문자로 변경할 수 있음
- 대표적 예제인 **WordCount**

#### 하둡의 성능, 사용현황
- **sort** : **map에서 reduce로 넘어가는 과정에서 항상 발생하는 내부적 프로세스**
- sort 작업이 데이터가 커질수록 **선형적으로 증가**
- 구성 서버를 늘린다고 처리 시간이 줄어들진 않음, 플랫폼이 자체적으로 선형 확장성을 가지고 있어야함
- sort는 플랫폼의 성능과 확장성을 동시에 측정할 수 있음

|구분|설명|
|:-:|-|
|야후|-하둡 프로젝트의 주요 후원자<BR>-4만대 이상의 컴퓨터에 하둡 설치, 가장 큰 클러스터는 약 4,500만개의 노드로 구성|
|야후의 WebMap|- 야후의 대표적 그래프 기반 검색 엔진<BR>-모든 edge 및 링크 정보를 계산해 결과를 다양한 애플리케이션에서 사용할 수 있도록 해주는 거대한 그래프<BR>-100개 이상의 MapReduce 작업들을 체인 형태로 묶어 실행, 압출해서 300TB가 나올 정도를 다루고 있음|
|국내|- NHN과 다음 등의 포털에서 하둡 사용<BR>-삼성SDS, SK등의 IT회사에서 하둡 활용|

<br>
---

### 병렬 쿼리 시스템
개요
- 일부 사용자들에게는 mapreduce가 쉽지 않음
- 병렬 처리 시스템 개발, 구글의 sawzall, 야후의 pig 등

#### 구글 Sawzall
- MapReduce를 추상화한 **최초의 스크립트 형태 병렬 쿼리 언어**
- MapReduce 생산성을 높임, 쉬운 병렬 프로그래밍
- Pig나 Hive도 Sawzall와 유사

#### 아파치 Pig
- Hadoop MapReduce 위에서 동작하는 추상화된 병렬 처리 언어, 아파치의 서브 프로젝트
- 전체 MapReduce작업의 약 30%에 Pig사용

개발 배경
- 실제 대부분의 업무가 한 번의 MapReduce로 끝나진 않음
- 또한 MapReduce를 이용하는 개발자들이 유사한 알고리즘을 중복 개발하는 경우가 많지만 의미 파악이 어려워 공유는 잘 이루어지지 않음
- 의미적으로 SQL과 비슷하지만 새로운 언어인 Pig를 정의

사용
- **MapReduce는 무공유 구조** -> join연산을 매우 복잡하게 처리(400라인)
- Pig를 사용하면 10라인으로 가능
- 검색 인프라, 광고 연관성 분석, 사용자 의도 분석, 검색엔진 쿼리 분석, 호프만 plsi 등 다양한 분야에서 이용

#### 아파치 Hive
= 페이스북에서 개발, pig와 마찬가지로 하둡 플랫폼 위에서 동작
- SQL기반 언어와 JDBC지원, Hadoop-Streaming을 쿼리 내부에 삽입해 사용
- **맵리듀스의 모든 기능을 지원**

개발 배경
- 페이스북은 초기에 DBMS기반의 시스템을 운영했으나 데이터가 수백TB규모로 늘어나 관리 및 비용 절감의 필요성 느낌
- DBMS를 하둡으로 교체하는 과정에서 필요한 기능들을 하나씩 구현하면서 만들어짐

아키텍쳐
- Metastore는 raw file들의 콘텐츠를 테이블 내 칼럼처럼 구조화된 형태로 관리할 수 있게 해주는 스키마 구조
- 별도의 DBMS를 설정하지 않으면 **Embedded Derby를 기본 DB로 사용**
- 앞 단에는 커멘트 라인 인터페이스(CLI)가 있는데 이걸로 SQL쿼리 사용
- 파서에서 쿼리를 받아 구문 분석을 하고 Metastore에서 정보를 참조해 Execution Plan을 만든다.
- Execution Engine은 하둡의 jobtracker와 네임 노드와 통신을 담당하는 창구 역할을 하면서 Mapreduce작업을 실행하고 파일을 관리함
- Serde라는 것은 Serilizer와 Deserializer의 줄임말, 테이블의 로우나 칼럶의 구분자 등 저장 포멧을 정의하는 컴포넌트

#### 하이브의 언어모델

|DDL|DML|Query|
|-|-|-|
|- 테이블 생성,삭제, 변경 명령<BR>-테이블 스키마 변경<BR>-테이블, 스키마 조회|-로컬에서 DFS로 데이터 로드<BR>-쿼리 결과를 테이블이나 로컬파일 시스템, DFS에 저장|SELECT, GROUP BY, SORT BY, JOINS, SAMPLING, TRANSFORM, SUB QUERIES, UNION|

---

<BR>

### SQL on 하둡
개요
- 실시간 처리라는 측면에서 하둡의 제약사항을 극복하기 위한 시도, **실시간 SQL질의 분석 기술**
- 대화형식의 SQL질의를 통해 처리하고 분석

#### 임팔라
- SQL on 하둡 기술 중 먼저 대중에게 공개된 기술
- **분석과 트랜잭션 처리를 모두 지원**
- 하둡과 Hbase에 저장된 데이터를 대상으로 SQL질의를 할 수 있다.
- 자바 대신 C++사용, 맵리듀스를 사용하지 않고 실행 중 최적화된 코드를 생성해 데이터 처리

#### 임팔라 구성요소

|구분|설명|
|:-:|:-|
|클라이언트|ODBC/JDBC 클라이언트, 임팔라쉘 등에 해당, 임팔라에 접속해 테이블 관리, 데이터 조회 등의 작업 수행|
|메타스토어|임팔라로 분석할 대상 데이터들의 정보 관리, 하이브의 메타데이터를 같이 사용|
|임팔라 데몬|시스템에서는 ImpalaD로 표시되며 클라이언트 SQL 질의를 받아서 읽기/쓰기 작업 수행, 질의 실행계획기, 질의 코디네이터, 질의 실행엔진으로 구성|
|스테이트 스토어|데몬들의 상태 체크하고 건강정보를 관리하는 데몬, 장애가 생기면 다른 데몬들에게 알려서 장애가 발생한 데몬에 질의가 가지 않도록 함|
|스토리지|분석할 데이터의 저장소, 현재는 Hbase, HDFS 지원|


#### 임팔라 동작 방식

- 모든 노드에 임팔라 데몬이 구동되고 사용자는 구동된 임의의 노드에 JDBC,ODBC, 임팔라쉘을 이용해 질의를 요청할 수 있다.
- 사용자의 질의는 데이터의 지역성을 고려해 노드 전체로 분산되어 수행
- 질의 요청을 받은 코디네이터 데몬은 분산되어 수행된 각 임팔라 노드들의 부분 결과를 취합해 결과값을 만들어 사용자에게 제공
- 실제 운영 환경에서는 **라운드 로빈 방식**으로 질의를 분산시켜 질의에 대해 전 노드들이 코디네이터 역할을 고르게 수행하도록 해야함
    - 라운드 로빈 : 여러 프로세스들이 돌아가며 처리되는 스케줄링 방식

#### 임팔라 SQL 구문
-임팔라는 기본적으로 하이브의 SQL을 이용하나 모든 하이브 SQL을 지원하는 것은 아니라 어떤 구문이 지원되는지 확인해야함

|항목|설명|
|:-:|-|
|DDL|CREATE TABLE/DB, ALTER TABLE, DROP DB/TABLE, SHOW DB/TABLE(조회)|
|DML|SELECT, WHERE, GROUPBY, ORDERBY, INSERT, OVERWRITE (변경,삭제 구문은 지원 안함)|
|내장 함수|ABS, ACOS(코사인값 반환), DAY, FROM_UNIXTIME, IF, CASE,ASCII, CONCAT|

#### 임팔라 데이터 모델
- 임팔라는 하둡 분산 파일시스템에 데이터를 저장하며, 어떤 저장포맷을 사용하느냐에 따라 처리 성능이 다름

|항목|설명|
|:-:|:-|
|로우 단위 저장|-하둡의 기본 파일 포맷인 텍스트나 시퀀스 파일은 로우 단위 데이터 저장 방식<BR>-하나의 칼럼을 읽든 전체를 읽든 동일한 디스크 입출력 발생|
|칼럼 단위 저장|- 읽고자 하는 칼럼만큼의 디스크 입출력 발생 -> 성능 개선(전체 칼럼을 읽을 땐 개선X)<BR>-처리 시간이 로우보다 적게 걸림, 다만 파일이 처음부터 칼럼 파일 포맷을 사용하지 않았을 때 파일 포맷 변경 작업을 해주어야 함<BR>칼럼 단위의 파일 저장 포맷인 RCFILE을 사용할 경우, 디스크 입출력 양을 현저하게 줄일 수 있다.|

<BR><BR>


## 클라우드 인프라 기술

### 클라우드컴퓨팅
- 가상화 자원들을 인터넷으로 서비스하는 기술
- Iaas, SaaS, PaaS의 3유형
- Iaas : 네트워크 장비, 서버와 스토리지 같은 IT인프라 자원을 빌려줌
- SaaS : 소프트웨어를 웹에서 사용할 수 있게 함
- PaaS : 애플리케이션이나 소프트웨어 개발 및 구현 시 필요한 플랫폼 제공
- **VMware, Xen, KVM등과 같은 서버 가상화 기술**은 IaaS에 주로 활용
- 아마존은 S3, EC2 환경을 제공함으로써 플랫폼을 위한 클라우드 서비스르 최초로 실현, **AWS의 EMR은 하둡을 온디맨드로 이용**할 수 있는 클라우드 서비스

#### 서버 가상화의 개념 및 특징
정의 : 물리적인 서버와 운영 체제 사이에 적절한 계층을 추가해 사용자에게 **물리적인 자원은 숨기고 논리적인 자원만을 보여줌**
특징
- 서버 가상화는 하나의 서버에서 여러 애플리케이션, 미들웨어, 운영체제들이 서로 영향을 미치지 않으면서 동시에 사용하도록 함.
- 서버 가상화를 가능하게 하는 기술은 다양하고 메인프레임, 유닉스서버, X86서버 등에 따라 다른 기술,분류체계가 사용됨

#### 서버 가상화 기술의 효과
- 가상 머신 사이의 데이터 보호
    - 다른 가상머신들 사이의 접속은 정상적인 네트워크 접속만 허용
- 예측하지 못한 장애로부터 보호
    - 장애가 다른 가상머신에는 전혀 영향을 미치지 않음
- 공유자원에 대한 강제 사용의 거부
    - 할당된 자원 이상을 가져가는 것을 차단함, 다른 가상머신에 할당된 자원 부족 현상을 차단함
- 서버 통합
    - 동일한 데이터 센터의 물리적 자원을 이용하면서 더 많은 서버를 운영할 수 있다.
- 자원할당에 대한 증가된 유연성
    - 전체 시스템 자원을 재배치해 자원 활용도 극대화 
- 테스팅
    - 새로운 서버를 추가하지 않아도 테스트 환경을 구성할 수 있음.
- 정확하고 안전한 서버 사이징
    - 사이징 예측이 불확실한 서버르 구성할 때도 일단 확보된 리소스를 이용해 할당 후 쉽게 추가로 할당할 수 있다.
- 시스템 관리
    - 마이그레이션 기능을 이용할 경우 운영 중인 가상머신의 중지 없이 다른 물리적 서버로 이동시킬 수 있다.
    - 하드웨어 장애, 로드 밸런싱, 업그레이드 업무를 쉽게 수행할 수 있음

### CPU 가상화
#### 하이퍼 바이저의 개념 및 특징
- 물리적 서버 위의 가상화 레이어를 통해 필요한 하드웨어 환경을 가상으로 만듬
- 하이퍼 바이저 : 호스트 컴퓨터에서 다수의 운영체제를 동시에 실행하기 위한 논리적 플랫폼
- 일반적인 가상머신은 하이퍼바이저(VMM)
- X86 계열에선 소프트웨어 기반

#### 기능
- 하드웨어 환경 에뮬레이션
- 실행환경 관리
- 시스템 자원 할당
- 소프트웨어 스택 보존

#### 하이퍼바이저 관련 기술 분류
1. 플랫폼별 분류
X86 : VMware,MS Virtuall Server, Xen
유닉스 계열 : IBM - POWER Hypervisor
메인 프레임 계열 : z/VM, PR/SM
2. 위치와 기능에 따른 분류
- 가상화를 제공하는 하이퍼바이저가 물리적 하드웨어 또는 호스트  운영체제와 관계에서 어디 위치하는지에 따라 베어메탈, 호스트 기반으로 나뉨
- 베어메탈은 하드웨어와 호스트 운영체제 사이에 위치, 호스트는 호스트 운영체제와 게스트 운영체제 사이에 위치
- 베어메탈은 반가상화, 완전 가상화로 구분
3. privileged 명령어 처리 방법에 따른 분류
- 최근에는 새로운 가상화 방법이 계속 나와 정확한 분류가 어려움
- x86 운영체제는 모든 하드웨어에 대한 제어 소유권을 가지고 있다는 가정 아래 하드웨어에 직접명령을 수행하는 방식으로 디자인됨
- x86 아키텍쳐는 Ring 0,1,2,3 등 4개 레벨로 구성, 운영체제는 0레벨, 사용자는 3레벨
- 운영체제가 3레벨로 수행될 경우 복잡한 문제 발생
- 3이 수행된 운영체제에서 0 수준 명령을 호출하면 이를 0 수준의 명령어로 다시 변환해 실행해야함
- **반가상화, 완전 가상화 용어도 privileged명령어를 어떻게 처리하느냐를 기준으로 분류**

#### 가상화 방식 분류
1. 완전 가상화
- 하이퍼바이저보다 우선 순위가 낮은 가상머신에서는 실행되지 않는 privileged명령어에 대해 trap을 발생시켜 하이퍼바이저에서 실행하는 방식
- VMware ESX Server, MS Virtual Server
- 최근 Xen에서 Intel vt-X, AMD-V환경에서 지원
- 장점
    - CPU뿐만 아니라 모든 자원을 하이퍼바이저가 직접 제어,관리하기 때문에 어떤 운영체제라도 수정하지 않고 설치 가능
    - MS윈도우와 같은 게스트 OS가 변경되지 않은 상태로 실행 가능
- 단점
    - 하이퍼바이저가 직접 자원 제어 -> 성능에 영향
    - 자원들이 하이퍼 바이저에 너무 밀접하게 연관되어 있어 동적변경 작업이 단일 서버내에는 어려움
    - 동적변경을 하기 위해서는 VMware의 VMotion과 같은 솔루션의 도움을 받아야 함
    - Para Virtualization에 비해 속도가 느림

2. 하드웨어 지원 완전 가상화
- 메모리와 CPU 등 하드웨어에 명령을 내릴 수 있는 반가상화 수준의 성능을 발휘
- CPU에 Ring-1 계층 추가, 하이퍼바이저가 ring-1에서 수행되고 가성머신의 os는 ring-0에서 수행되어 previleged명령어에 대한 변환과정이 필요없다.
- 빠른 성능, 윈도우2008의 Hyper-V는 반드시 가상화 지원 CPU만 써야함
- 인텔에서는 CPU사용률이 높아져 서버 통합을 목적으로 할 경우 비효율적이라고 제시함.
- 인텔에서 반가상화와 하드웨어 지원 완전 가상화를 모두 사용하는 하이브리드 가상화 제시
- Xen의 하이브리드 가상화의 경우 명령어의 종류에 따라 반가상화와 완전 가상화를 선택함

3. 반가상화
- **previleged 명령어를 게스트 운영체제에서 hypercall로 하이퍼바이저에 전달하고 하이퍼바이저는 hypercall에 대해 previleged 레벨에 상관없이 하드웨어로 명령 수행**
- hypercall : 게스트 os에서 요청하면 하이퍼바이저에서 바로 명령을 실행하는 call
- hypercall을 요청하기 위해서는 게스트os 일부분이 수정되어야 함(Xen에서 20% 커널이 수정됨)
- 반가상화 기반에서는 CPU와 메모리 자원의 동적 변경이 서비스 중단 없이 이루어질 수 있고 완전 가상화보다 성능이 뛰어남.
- 속도는 빠르나 커널을 변경해야하고 완전 가상화는 커널 변경은 없다.
- VMware같은 상용 솔루션은 완전 가상화와 반가상황의 단점을 보완해 뚜렷한 차이가 없음
- VMI라는 표준 인터페이스를 제시해 모든 게스트OS를 지원하는 체계로 반가상화 지원
- VMI는 아직 정식은 아니지만 리눅스 진영에서 도입하려는 움직임이 있음.

4. Monolithic vs Microkernel
- 드라이버가 어느 계층에 있느냐로 나뉨

|Monolithic 방식|Microkernel 방식|
|-|-|
|-가상머신이 I/O를 위해 하드웨어에 접근할 때 드라이버를 하이퍼바이저 계층에서 모두 갖고 있는 방식<br>-VMware의 경우 하이퍼바이저가 드라이버를 갖고 있고 모든 I/O요청은 하이퍼바이저가 수행<br>-성능은 조금 향상될 수 있지만 많은 코드를 가져서 장애가 발생할 가능성이 높다.|-I/O를 위해 하드웨어에 접근할 때 드라이버를 각 가상머신에서 가지는 방식<br>-Xen에서 하이퍼바이저는 드라이버가 없고 호스트OS가 가지고 있음. I/O 요청을 위해선 호스트OS를 거쳐야 함<br>-게스트와 호스트 OS는 서로 격리되어 있어 하이퍼바이저를 이용해 요청을 주고받음<br>속도는 느리지만 하이퍼바이저 계층이 간단하여 하이퍼바이저 변경이 필요없고 장애 확률이 낮다.|

<br>

#### 하이퍼바이저 기반 가상화 기술 비교

|구분|완전 가상화<br>CPU 기술 이용X|완전 가상화<br>CPU 기술 이용|반가상화|
|:-:|:-:|:-:|:-:|
|사용기술|바이너리 변환<br>Direct Execution|Privileged Instruction은 Ring-1로 처리|hypercall가능하게 게스트os변경<br>호환성 안좋음|
|게스트os<br>반응/호환성|게스트os 변경 없음<br>호환성 뛰어남|게스트os 변경 없음<br>호환성 뛰어남|hypercall가능하게 게스트os변경<br>호환성 안좋음|
|성능|좋음|Fair<br>(바이너리 변환 방식의 성능에 근접)|특정 경우에 좋음|
|성능|VMware, Microsoft, Parllels|VMware,Microsoft,Parllels,Xen|VMware,Xen|


5. 호스트 기반 가상화

- 완전한 os가 설치되어 하이퍼바이저가 호스트 os위에 탑재되는 방식
- 제약 사항이 많고 단일 os의 취약성이 있다.(신뢰성 문제)
- VMware, Workstation, Microsoft Virtual PC
- 테스트용 환경, 최근 사용x

6. 컨테이너 기반 가상화

- **운영체제만을 가상화한 방식**
- 가상화 지원계층을 하이퍼바이저가 아닌 **가상 운영환경**이라 부름
- Virtuozzo, Solaris Containers, Linux-VServer
- 장점
    - 하이퍼바이저 가상화 방식에 비해 훨씬 적게 가상화
    - **가상화 수준이 낮아 빠른 성능**
    - 한 대의 서버에서 더 많은 컨테이너를 실행할 수 있음
- 단점
    - 격리 수준이 낮아 다른 가상 운영체제가 영향받음
    - 보안 취약성에 의해 모든 가상 os에 문제가 생길 수 있음
    - 호스트os를 공유하기 때문에 **호스트 os문제가 전체 가상 os에도 영향 미침**

### 메모리 가상화 : VMware 기법

개념
- 가상의 공간을 만들어주는 프로그램
- 물리주소 : 0부터 실제 물리적인 메모리 크기까지를 나타냄
- 가상주소 : 하나의 프로세스가 가리킬 수 있는 최대 크기(32비트에서 4GB)
- 프로그램에서 주소는 가상주소값 -> **가상주소값 위치와 물리적 주소값 위치 매핑 과정 필요**
- **TLB : 매핑 연산을 하드웨어적으로 도와주는 것**
- 하이퍼바이저 내에 **Shadow Page Table**을 따로 두어 중간 변환 과정을 가로챔
- 모든 가상머신들이 자신만의 메모리 주소 공간을 갖도록 함
- 메모리 할당 문제를 해결해야함

#### 가상머신 메모리 할당의 문제 해결 방법
1. Memory ballooning
    - VMkernel은 가상머신의 메모리 영역을 **빈 값으로 강제로 채워 가상머신os가 자체적으로 swapping하도록 함**
    - 물리적인 메모리가 채워지고 있다는 것을 감지한 가상머신os는 swap파일에 메모리 영역을 page out시키고 자리를 비우게 됨.
    - 하이퍼바이저는 page out된 메모리 영역을 다른 가상머신에 할당함
2. Transparent page sharing
    - 하나의 물리적 머신에 여러 개의 가상머신이 운영되는 경우 각 가상머신에 할당된 메모리 중 **동일한 내용을 담고 있는 페이지는 물리적 메모리 영역에 하나만 존재시키고 모든 가상머신이 공유하도록 함**
3. Memory Overcommitment
    - 2GB 메모리를 가진 물리적 장비에 512GB를 Mininum reserved를 가질 수 있는 가상머신 5개를 수행할 수 있음
    - 앞의 2가지 기법으로 가능하지만 모든 가상머신이 메모리 사용이 많은 업무를 수행하는 경우면 심각한 성능저하 현상이 발생할 수 있어 권장하진 않음.

### I/O가상화

- I/O병목현상이 가장 문제가 됨 -> I/O자원의 공유 및 파티셔닝이 필요
- 또한 가상머신 간에도 통신이 이루어져야 함. 이를 위해 다양한 기술이 사용
    - 이더넷 : IEEE가 표준 사양으로 택한 LAN에 사용되는 모델로 근거리 통신망 하드웨어, 프로토콜, 케이블 표준

1. 가상 이더넷
    - 가상 이더넷은 **물리적으로 존재하지 않는 자원을 만들어내는 에뮬레이션 기능 사용**
    - 각 가상머신 사이에 네트워크 어댑터 없이도 메모리 버스를 통해 고속 및 고효율 통신이 가능
    - 가상 LAN기술을 기반으로 한 네트워크 파티션도 가능하게 함, 각 가상 LAN 사이에는 통신을 할 수 없음
    - 사용자들은 **별도의 물리적 어댑터와 케이블을 사용하지 않고도 네트워크 이중화, 네트워크 안전성 단절 등의 효과를 얻을 수 있음**

2. 공유 이더넷 어댑터
    - 여러 개의 가상머신이 **물리적 네트워크 카드를 공유할 수 있게 하고 외부 네트워크와 통신이 가능하게 함**
    - 가상 머신 개수보다 물리적 어댑터 개수가 적은 경우 가상머신들이 물리적 이더넷 어댑터를 공유할 수 있게 해줌
    - 하나 자원을 여러 가상 머신이 공유해 **병목현상을 피할 수 없음**
    - 최근에는 네트워크 어댑터 내에서 가상화를 지원하게 함

3. 가상 디스크 어댑터
    - 파이버 채널 어댑터와 같은 I/O어댑터의 부족

**가상화된 환경에서 가상 디스크를 이용해 가상머신이 디스크 자원을 획득하는 방법**

|내장 디스크|외장 디스크|
|-|-|
|-가상 I/O레이어가 내장 디스크를 소유하고 있고 이 것을 논리적 디스크 드라이브로 나눈다.<br>-나누어진 드라이버는 LUN으로 각 파티션에 가상 디스크 어댑터를 통해 분배됨<br>-이렇게 획득한 논리적 디스크 자원을 물리적 자원처럼 인식|-먼저 가상 I/O레이어가 파이버 채널 어댑터를 통해서 외장 디스크의 LUN을 획득<br>-내장 디스크와 달리 가상 I/O레이어가 바로 각 가상머신에 가상 디스크 어댑터를 통해 분배<br>-가상 I/O레이어를 통해 제공도니 논리적 디스크 볼륨은 이를 이용하는 다른 가상머신에게는 SCSI 디스크로 나타냄|
